%not in details put in introduction

\section{Introduction}

The main goal of this research is to distinguish all dynamic objects present in the scene. The scene is observed from a camera installed in a car.

This setup brings one issue: a priori all the objects are moving, since the car moves in the scene, thus the entire scene seems to change in every instant. 

When we talk about distinguish static and dynamic object, we are refeering them to be dynamic or static state with respect to the earth, not to the vehicle.

\section{demonstrator} %configuration of the lasers, diagram
\label{sec:demonstrator}

Our testbed used is a Toyota Lexus car equipped with two LIDAR lasers scanner (Section~\ref{sec:testbed} for the specification) installed in the frontal bumber (Figure~\ref{fig:demonstrator:birdeye}).

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.4]{img/fig:demonstrator:birdeye}
     \end{tabular}
   \caption{Bird-eye view: LIDAR laser scanners location}
   \label{fig:demonstrator:birdeye}
\end{figure}

The LIDAR is distributed in layers (Figure~\ref{fig:demonstrator:lateral}) and scanning range (Figure~\ref{fig:demonstrator:superior}). 

There are four layers in total, each layer disposed in different angles. The terrain is considered as base for the formation of this angle. This angle is constructed by the angle formed between an imaginary line, which is parallel to the road surface, and the layer, those angles are depicted in the Figure~\ref{fig:demonstrator:lateral}.

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.5]{img/fig:demonstrator:lateral}
     \end{tabular}
   \caption{Lateral view: Four LIDAR laser scanner layers}
   \label{fig:demonstrator:lateral}
\end{figure}

Every layer spread in a given range, 

From beams individually is possible to obtain the distance between the LIDAR laser scanner and the first object to intercept this beam of light.

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.5]{img/fig:demonstrator:superior}
     \end{tabular}
   \caption{Bird-eye view: Left LIDAR laser scanner range, for the first layer}
   \label{fig:demonstrator:superior}
\end{figure}


\begin{table}
	\begin{center}
	    \begin{tabular}{ | c | c | c | c | c |}
		    \hline
		    Layer & $+50^\circ$ & $+35^\circ$ & $-50^\circ$ & $-60^\circ$ \\ \hline
		    4 & + & + & + &  \\ \hline
		    3 & + & + & + &  \\ \hline
		    2 &  & + & + & + \\ \hline
		    1 &  & + & + & + \\ \hline
		    $\cap$ &  & + & + &  \\ \hline
	    \end{tabular}
	\end{center}    
    \label{tab:beam:interception}
    \caption{Beam spreading and layers interception}
\end{table}



\section{Sensor pre-processing} %fusion module

% in the last subsection, output form (occupancy grid), digrams; explains that this is our input

\section{The algorithm}
%motion detection

\subsection{Principle} 

\textit{Frame} is a snapshot of the current environment representation, it is used as one of the inputs required for the algorithm developed in this work. This algorithm requires the minimum amount of two \textit{frames} to work. 

But before jump on explanations on how those two frames are going to be used, we need to describe how those frames are obtained and how they mimic the environment and off course its limitations.

In the Section~\ref{sec:demonstrator}, we saw that our test platform is composed by two scanners, each of them containing several layers. Every layer has a certain number of beams, which are spread horizontally within a regular angle interval. 

So, before having a frame that can be used by our algorithm, its required to perform the fusion of all those beans, which is done in the \textit{}. 

\subsection{Occupancy Counting} 

\subsection{Bringing old frame up to speed}


\section{summary}

%summary