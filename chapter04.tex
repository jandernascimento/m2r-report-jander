\section{Algorithm}

In this section we are going to diplay all very same formulas seen in previous sections but as algorithms, which complements the understanding of previous sections.

\subsection{General method}

In the Algorithm \ref{al:general}, we provide an overview of the framework created for detecting moving parts.

\begin{lstlisting}[title={Algorithm},mathescape=true,label=al:general,caption={General framework algorithm for motion detection}]
//keeps the occupancy grid for t and t-1, for that reasor 2 position in one of the dimentions
struct MotionData { $timestamp$, $velocity_x$, $velocity_y$, $quaternion \{q0,q1,q2,q3\}$ }
struct u_t { $speed$, $angular$_$speed$, $dt$ }
$OG_t$[N]
$OC_t$[N] $OC_{t-1}$[N]
$FC_t$[N] $FC_{t-1}$[N]
$MotionData_t$ $MotionData_{t-1}$

DetectMotion() 

	while not quit do 
		$OG_t$ = ReadAndFuseLaserData() //write about it
		$MotionData_t$ = ReadMotionData()
		$u_t$=FindMovement($MotionData_t$,$MotionData_{t-1}$)
		InitCounterArraysForNewData($OG_t$,$FC_t$,$OC_t$)
		UpdateCounterArraysFromPrevious($u_t$)
		$MotionData_{t-1}$=$MotionData_t $
		$OC_{t-1}$=$OC_t$
		$FC_{t-1}$=$FC_t$
		DataMotion($OG_t$)

FindMovement($MotionData_t$,$MotionData_{t-1}$)
	//explained in next sections

InitCounterArraysForNewData($OG_t$,$FC_t$,$OC_t$)
	//explained in next sections

UpdateCounterArraysFromPrevious()
	//explained in next sections
	
\end{lstlisting}

The elements from the Algorithm \ref{al:general} are defined in the high level of abstraction. In the next sections we are going to explain the calls made in this code.

All variables declared out of the function scope are considered as global variables, but for better understanding of the dependencies among the functions, the input data are going to be sent by parameter in some situations.

\subsection{Finding the movement}

\begin{lstlisting}[label=al:movement,mathescape=true,caption={Find the movement of the vehicle}]
FindMovement($MotionData_t$,$MotionData_{t-1}$)
	//velocity vector from horizontal and vertical speed	
	velX=$MotionData.velocity_x$
	velY=$MotionData.velocity_y$
	$u_t.speed$=sqrt(velX*velX+velY*velY)
	
	//quaternion to euler angles conversion	
	Q=$MotionData_t.quaternion$
	yaw=atan2(2(Q.q0*Q.q3+Q.q1*Q.q2),1-2(Q.q2*Q.q2+Q.q3*Q.q3))	

	Q=MotionData_{t-1}.quaternion
	yaw2=atan2(2(Q.q0*Q.q3+Q.q1*Q.q2),1-2(Q.q2*Q.q2+Q.q3*Q.q3))	
	
	$u_t$.dt=$MotionData_t.timestamp$-$MotionData_{t-1}.timestamp$
	
	$u_t$.angular_speed=(yaw2-yaw)/$u_t.dt$
	
	return $u_t$
\end{lstlisting}

This algorithm is used to calculate the $u_t$, mathematically explained previous sections, $u_t$ 

\subsection{Initializing Grid}

\begin{lstlisting}[label=al:initializeGrid,mathescape=true,caption={Initialization of the grid}]
InitCounterArraysForNewData($OG_t$,$FC_t$,$OC_t$)
		for i=0 to N-1 do
		if OG[i] > 0.5 then 
			OccupiedCounter[i]=1
			FreeCounter[i]=0
		else if OG[i] < 0.5
			OccupiedCounter[i]=0
			FreeCounter[i]=1
		else
			OccupiedCounter[i]=0
			FreeCounter[i]=0
\end{lstlisting}

At this point, we initialize the counters (occupied and free) based on the informations available in the current Occupancy Grid $OG_t$.

\subsection{Updating Counter arrays}

\begin{lstlisting}[label=al:updatecounter,mathescape=true,caption={Update counter arrays}]
UpdateCounterArraysFromPrevious()
	newPose=FindPoseOfNewGrid($u_t$)
	repose=FindInversePose(newPose)
	for i=0 to N-1 do
		$P_i$=PoseOfCell_i in $OG_{t-1}$
		$P_j$=CompoundPose(repose,$P_i$)
		$cell_j$=CellFromPose()
		If $cell_j$ is not visible in $OG_t$ then
			continue
		$O_t$[$cell_j$]+=$O_{t-1}[cell_j]$
		$F_t$[$cell_j$]+=$F_{t-1}[cell_j]$
\end{lstlisting}

In this function we integrate new data acquisition from the LIDAR to the current counters. Some of the functions will not be detailed, specifically those in which the translation between mathematical and algorithms perspective do not change considerably.
The functions $FindPoseOfNewGrid$ applies the Equation~\ref{eq:circularmotion}, \ref{eq:pose:compound}  and \ref{eq:pose:compound:inverse}. The function $CompoundPose$ comes from the Equation~\ref{eq:composepose}. $CellFromPose$ is mapped from Equation~\ref{eq:grid:cell:calculation}.

\section{Experiment}

In the experimentation phases, we modeled the algorithm to compute ahead of the time by using the bicycle model and extrapolating the pose by using the current vehicle measurements, like stearing angles. But this measurement had shown a lot of variation during a run, and the extrapolated model gets out to date very quickly.

From mechanical physics we obtain the angular velocity and from the vehicle \emph{CAN bus} we obtain the data time stamp, and along with it the time variation, $\Delta t$.

Using transformational matrices we bring the car perspective \cite{iyengar1991autonomous} and the prediction to the same frame of reference.

\section{Evaluation}

%target: dynamic and static separation, reduce the number of frames required, no classification, no association; no tracking, no 

\subsection{Testbed}
\label{sec:testbed}

\subsubsection*{Hardware}

For testing purposes, we have a Lexus LS600h equipped with a stereo vision (TYZX camera) positioned in the superior part of the windshield.

The vehicle has two IBEO Lux Lidar in the front side of the car, one in each corner of the bumper.

As IMU, the car is equipped with a Xsens IMU and a GPS for global positioning.

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
     \multicolumn{2}{c}{ \includegraphics[width=0.55\columnwidth]{img/testbed:car}}\\
       \includegraphics[width=0.40\columnwidth]{img/testbed:ibeo}
       &\includegraphics[width=0.40\columnwidth]{img/testbed:tyzx}
     \end{tabular}
   \caption{Lexus LS600h car equipped with two IBEO Lux lidars and a TYZX
     stereo camera}
   \label{fig:Lexus}
 \end{figure}

As the test platform requires a powerfull computer to process the stereo images in realtime, the car is equipped with a Dell workstation with an NVidia graphic card.

DeepSea TYZX stereo vision camera has a base line of 22 cm with 512x320 pixels of resolution and focal length of 410 pixels, this camera is provided with a library which can estimate the distance in cm of the objects captured by the camera, but in our tests we used the estimators developed at Inria\cite{PERROLLAZ-2010-493397}.
%better references
%http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=6170895
%http://hal.inria.fr/index.php?halsid=fa9c2loot97ptftt2n38f6lvr5&view_this_doc=hal-00671208&version=1

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[width=0.45\columnwidth]{img/testbed:xsens}
       & \includegraphics[width=0.45\columnwidth]{img/testbed:trunc}
     \end{tabular}
   \caption{MTi-G XSens IMU unit \& Intel Xeon 3.4GHz linux box}
   \label{fig:Lexus}
 \end{figure}

Each IBEO Lux Lidar is composed of four layers, each of them providing about 200 beams. The angular range of 100 degrees with angular resolution of 0.5 degrees. Each beam can reach until 200 m of distance measurement, with width of 40 m and maximum height of 2 m.

MTi-G XSens minimum of 120Hz and maximum of 512Hz for data logging and angular resolution of 0.05 degrees, all specifications are valid when considering an homogenous eletromagnetic environment. Maximum altitude operation is 18 Km and maximum speed is 515 m/s.



\subsubsection*{Software}

The high end cars are composed of several internal networks, each of them responsible to handle the transmission of the informations that are captured in that network to other one which need to process that information. This type of network is known as Controller Area Network (CAN) and provides a cheap and reliable way to communicate with devices, so CAN bus have been replacing the regular point-to-point wiring interconnection among the components\cite{bosch91can}.

Car manufacturers use platforms like YARP (Yet Another Robot Platform) URBI or ROS to publish the information gathered by those networks, frameworks like ROS, allow the car to push information collected by the CAN bus into a shared memory array which can be access by other applications, the intent for most of those frameworks is to give longevity\cite{Fitzpatrick:2008:TLR:1327539.1327705} for the robot application that uses it by providing a platform that can evolve in cooperation manner with other projects.

%For this project Robot Operating System(ROS) was adopted, ROS gives us the hability to save all information obtained during a run (driving the test platform) from all sensors, and replay it later in the laboratory, this by far allow to have a consistent test samples and a historical evolution of all scenarios gathered during the algorithm evolution, alowing the retro-performance benchmarking with other versions of the algorithm.

\subsubsection*{HugrStore}

Data Distribution Service\cite{dds} is a set of specifications for data distribution in real-time systems, this specification is maintained by OMG Data Distribution SIG (DDSIG).

DDS works in a publish-subscriber manner, where the subscriber acquire the data by subscribing to one or several  "topics", which is the name assigned for a data provider (the publisher itself). The goal is to promote the lose coupling and real-time data sharing between applications.

Now that we are aware of DDS specification, we can talk about HugrStore, which is the implementation chosed. Thus, hughr is a middleware that allows sharing information among applications using a shared memory segment, in a publisher-subscriber manner. Hugr is DDS compliant. The communication model is represented in the Figure \ref{fig:dds:hugr}.

\begin{figure}[H]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.50]{img/fig:dds:hugr}
     \end{tabular}
   \caption{Publisher-Subscriber scheme}
   \label{fig:dds:hugr}
\end{figure}


\subsubsection*{Assumptions}

\textit{Goal: What we assume to make our algorithm work}

\subsubsection*{Notation}

\paragraph{Occupancy Grid visual representation}

Representating the occupancy grid, implies in taking some precautions in the pattern used to represent each state, in this work the value $1$ represents occupied spaces and $0$ free spaces, although in the visual representation of the occupancy grid the $black$ dots represent occupied spaces and $white$ ones represent free spaces. In certain occasions, where the grid depicted is not binary, spaces where framework has absolutely no information about certain spaces (\textit{e.g.} due unreachability of the perception sensors in that sector), the color used is $gray$.

\paragraph{Uncertain spatial relationship notation} In this document we are using the same notation as applied in the work \cite{Wang04a}. With:

$\oplus$ compounding operation.

$\ominus$ inverse operation.

\subsection{Results}

\textit{Goal: What are our current results}

\subsection{Conclusion}

\textit{Goal: Express what we can conclude from this work, talk about future work, may be}