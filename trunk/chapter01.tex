\section{Driving support systems}

In this document we are mainly concerned about driving assistance systems. Such system facilitates the task of driving, this support can appear in several forms: 

\begin{itemize}
\item visual alerts
\item auditory alerts
\item tactile alerts (haptics\cite{riener2010sensor})
\end{itemize}

Driving support system may provide support in a less passive manner as well:

\begin{itemize}
\item change the car responds to the drivers commands
\item change the car behaviour
\end{itemize}

This field has gained several adepts, auto-mobile industry and researchers have studied it throughout the years.

Thus, driving support systems and its applications are becoming a popular tool. To exemplify such change, some countries changed its law to make mandatory that all new cars manufactured have a given support system installed as a minimum equipment, ABS(Anti-lock break systems)  for instance.

One of the reasons for such change is the increasing number of incidents involving vehicles. \textbf{Risk of incidents} is growing as the population of cars grows, and there is no signal in changing those numbers in next years, so driving support systems is coping with task of making the car a safer transportation system, among others tasks \ref{fig:sensor:target}.

Those incidents may be related with the cognitive capacity limitation of the human being. With the goal of quantify this capacity, a research conducted in 2008 studied the human cognitive limitation for visual sensing mechanism \cite{LautarutisV}. This report established the limit number of objects in which the human is capable of track simultaneously. 

Although this seems to be distant from our goal (safety), this research underlies the limitation of human driving skills. With it we can conclude that above certain number of surrouding vehicles, we may start to ignore the risk of collision with some vehicles that still may represent a risk.

%Although, cognition is not an absolute number it varies according to several parameters, like mental health, type of sensing mechanism (visual, auditory, tactile), amount of information transmitted, etc. 

The action of driving did not change since its creation but several mechanisms to support the driver have been included in the vehicle. All to provide a better and safer experience for the driver\cite{riener2010sensor}.

A more modern approach of driving support system has appeared: Advanced Driver Assistance Systems (ADAS). 

ADAS relies in computerized systems to process informations provided by the car (through sensors) to improve the user experience. 

\textbf{ADAS} is splitted in different research branches, each of them with its own application and goals. Among those different branches (Figure \ref{fig:sensor:target}), this work will concentrate in applications situated in \textbf{Safety} branch. 

The goal of this work is to tackle a very common issue in safety application (object classification) and \textbf{not} develop an application.

This will be done by providing a theorerical framework to classify the environment in a high level manner, separating static and dynamic objects \textit{a priori}.

\begin{figure}[h]
\centering
	\begin{tabular}{lr}\\
		\includegraphics[scale=0.7]{img/fig:sensor:target} 
	\end{tabular}
	\caption{ADAS application stack and problem to be tacked (adapted from \cite{riener2010sensor})}
	\label{fig:sensor:target}
\end{figure}

Advanced Driver Assistance Systems must to perceive the environment before acting. ADAS works just like the human cognition, it requires a mean to perceive the environment before dispatch any action. 

There are different ways of perceiving the environment, usually we observe its aspects, like: lighting, appearance, etc. Each one of those aspects have its own "observer": the sensor.

Every aspect requires a mechanism to convert physical characteristics into electronic signal that can be read by other equipments. The peripherals responsible for that is called \textbf{sensors}, which will be discussed in more details in the Section \ref{sec:sensors}.

%% reviewed until here

\subsection{Sensors}
\label{sec:sensors}

%maybe replace the world system in the diagram and text by "control systems"

Sensing is input for numerous biological systems (\textit{e.g.} nervous system, digestive system), enabling such systems to respond properly to the changes observed by our sensors(eye, skin, etc.). 

This cooperation is as important to robotics as for they are for human being. In order to interact with the environment, a robot, needs first to perceive it. We do that by capturing its aspects, or at least some of them. The aspects that are going to be observed depends on the application needs.

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.45]{img/fig:sensors:role}
     \end{tabular}
   \caption{The general architecture of a system attached to sensors}
   \label{fig:sensors:role}
 \end{figure}

Sensors do not perform action. They are the means in which complex systems acquire informations about the environment. Although, the sensors play a key role in almost every systems, they are closer to tha hardware layer (Figure \ref{fig:sensors:role}), meaning that it is not they role to process information, only acquire them. 

The sensors are normally, incapable to respond to external stimulus without a system to observe the data provided by it, and act. The system react to the sensor stimulus through other means.

Thus, sensor is responsible for the data acquisition (electric interpretation of a physical stimulus). This data is directed to a perception layer which is allocated in an upper layer.

\subsubsection{Types}

There are virtually sensors for all kinds of properties (Figure\ref{fig:sensors}) that we want to observe. In sometimes literatures those properties can be called \textbf{measurands}. 

The sensors are classified based in which the measurands they were designed to observe.

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.70]{img/fig:sensors}
     \end{tabular}
   \caption{Different types of measurands (adapted from \cite{WhiteRichard})}
   \label{fig:sensors}
 \end{figure}

\subsubsection{Properties}

The sensors are subject to issues, like any other electrical device. As the sensor is the capacity of a system to observe the environment, this affects directly the solution (the main purpose of its existence).

The most common issues are: noise, imprecision. 

\paragraph{Noise} depending on the mean used by the sensor to perceive the physical events, it may become more or less subjected to interferences. Those interferences can cause instability in the data acquisition, possibly leading to its miss interpretations.

\paragraph{Imprecision} sensors rely on physical properties of the observed \textbf{measurands}, those properties may vary according to the environment in which the sensor is deployed.

Light Detection and Ranging (LIDAR) can be a good example of imprecision. LIDARs uses light to measure the distance from objects, it does this by counting the time taken to the light hit the object and be reflected back. Although in the space the speed of the light is constant, in earth it depends on density of the atmosphere, amount of water particles in the air, among others. All those properties can change the speed of the light, thus, influencing dramatically the measurements.

\section{Perception}

Before talk about perception, its necessary to comprehend few concepts. Those concepts will frequently appear during this work: \textit{localization} and \textit{mapping}.

Localization provides the coordinate of a point with respect to a static reference, this gives to this point a spatial positioning where an system actor (ego-robot) is using some reference which belongs statically to the environment.  

Mapping, as general knowledge, gives us information about a place, this representation gives as nothing but the static objects position. This might be confusion at some point, but this static objects positioning is provided using as reference other statics objects, and this is the key for this understanding.

By putting these two concepts together, we can go even further. Imagine the following situation, we have a map in our hands (provided by the environment \textit{mapping}) and we know exactly where we are in this map (by \textit{localization}), what would be the result of moving in this environment blind folded? Not good.

For the exactly same reason we need constantly to have the location of the moving object in the very same map, this is called \textit{tracking of moving objects} or even just \textit{tracking} \cite{Wang04a}.

In the next sessions we are going to see in more details the approaches utilized by robotics research to tackle the localization and mapping issues.

\subsection{SLAM}

SLAM (Simultaneous Localization and Mapping) is an approach that tries to solve concurrently the \textit{mapping} and \textit{localization} issues\cite{VU-2009-454238}. In this approach its assumed that the the initial position of the ego-robot is unknown. 

In such approach,  the observation acquired by the robot's sensor should be used. The sensor reading should be enough to obtain the robot's \textit{localization}. That localization coordinates (one or several in case of uncertainties) must be visible in the current map of the environment. 

Although, this model requires a precise \textit{mapping} of the environment. This precision is required duo to parallel actions performed by the robot: moving itself in the (which change the environment from the robots perspective) and localizing the robot at the same time, and as the robot motion is performed in a continuous space, any interference in the measurements are accumulated and can affect severely the localization, diverging the calculated position from the real position. The Input-processing-output scheme can be seen in the Figure \ref{fig:perception:slam}.

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.9]{img/fig:perception:slam}
     \end{tabular}
   \caption{SLAM process(adapted from \cite{Wang04a})}
   \label{fig:perception:slam}
\end{figure}

The robot actions are subject of physical interferences as well. Depending in the kind of terrain in which the robot is moving affects how the robot movements can be trusted \cite{DBLP:conf/icra/LenainTHM11}. 

Thus, \textbf{inertia} plays a key role in the reliable the robot movements are. Other properties affects \textbf{inertia} such as: balance, weight distribution, grip condition, type of movement, etc.

All those variations bring an imprecision factor into the movement performed by a robot. This because those properties determines the resistance of the robot in changing its speed.

From the probabilist point of view, this implies in estimate the joint probability distribution (Equation \ref{jpd:discrete}). Where the $u$ represents the observations and $z$ the measurements.

Thus, the output of the SLAM process is the robot pose and a map of the stationary objects captured by the \textit{perception} sensors \cite{iyengar1991autonomous}.

\subsubsection{Formalization}

In SLAM, our main concern is to localize ourselves and the static objects in the surrounding environment. 

Intending to simplify the perspective of this problem, we will generate a scenario that will be used in analogy to the formula and may help the reader to absorb the intuition behind the formalization.

\paragraph*{Lost Robot} is positioned arbitrarily in a finite unknown space. This is a classical 4 wheeled robot. This same robot is subject of any kind of random forces and terrain condition. 

Suppose we send a command to the robot to move one centimeter ahead, how do we ensure that he really changed one centimeter? Suppose that without observing the last position of the robot we send another command to move another centimeter ahead, what would be the consequences? 

Every time we actuate in the robot without checking/knowing the real position, the uncertainty grows, and the possible new position that can be assumed by the robot increases exponentially.

Formalization of such dependency can be done through \textit{Bayes rule}. Bayes rule define the \textit{posteriori} probability distribution over a set $X$, normally represented as $p(A | B)$, which can be read as probability of $A$ knowing $B$.

\begin{equation}
\label{jpd:discrete}
P(x_t,M | z_{0:t}, u_{1:t})
\end{equation}

\subsection{DATMO}

Detection and Tracking of Moving Objects (DATMO) was initially studied by radar tracking systems \cite{VU-2009-454238} researches. Since in RADAR only moving objects are visible.

Those objects in scene they must solve the data association problem. The former assumption is unrealistic in the vision of a robot, the perception sensor senses all objects, independently on their motion, so static and dynamic objects are included in the representation.

As DATMO is a large process, some researchers chose to pick just a part of this process, which is "Moving Object Tracking" and solve it, without worry about the detection phase.

DATMO responsibility is to:

\begin{itemize}
\item Detect and initialize new objects
\item Remove objects that went out of the scene
\item Moving object motion modeling
\item Data association
\end{itemize}

\subsection{SLAMMOT}

Researches considered SLAM and DATMO as two different problems that should be solved separately, \textit{Wang} was one of the first researches to put in evidence the similarities between both problems \cite{Wang03onlinesimultaneous}.

As result, the SLAMMOT term was coined and a derivation of the SLAM formula with DATMO was created to simplify the process of tracking. The experiment showed that solving the SLAM with DATMO increased dramatically the performance of the algorithm comparing them with the results as individual solutions (performance was verified in crowded urban environment).

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.9]{img/fig:slammot}
     \end{tabular}
   \caption{SLAMMOT composition (adapted from \cite{Wang04a})}
   \label{fig:slammot}
 \end{figure}

As the GPS system or an good IMU for a good precision are too expensive, the paper propose a bayesian approach solution which satisfies the navigation constraints without compromise the safety and at the same time are not high costly as the other solutions based on a specialized hardware.

\subsection{SLAM and DATMO}

Robots face a set of problems which are common among many robots, how to move in a certain environment is one of them.

The general environment perception (Figure \ref{fig:perception:cycle}) requires at least two inputs: the perception measurements $Z$ which are usually are provided by cameras, LIDAR, or even radars, and motion measurements provided by the IMU (Inertial Measurement Unit).

\begin{figure}[h]
   \centering
     \begin{tabular}{lr}
       \includegraphics[scale=0.5]{img/fig:perception:cycle}
     \end{tabular}
   \caption{Perception cycle}
   \label{fig:perception:cycle}
 \end{figure}


He perceives the environment by using as input sensors informations and can be composed of several sensors, or even heterogeneous types of sensors, and those give a partial observation of the environment (according with the current technology applied in those sensors). Thus, the robot does not known what paths can be perceived to reach a given point in a space.

So, knowing the positions that can be assumed by the robot is essential and necessary so robot cat move in this environment, this is done by observing the environment through the sensors and creating a spatial relation between the static objects in this space, this is known as SLAM - or Simultaneous Localizations and Mapping \cite{iyengar1991autonomous}.

Although, this just concern a small set of the problems related with robots applications, for instance robots that are used for mining, in this type of environment the only element that can change the environment and the spatial relation between the static objects is the robot itself. 

Depending in the goal of the robots perception, may be required to make distinction between each object, with cameras we can use characteristics like color, shape. Whereas, classify an object based in a LIDAR or radar information neither shape nor color are available, at least not directly without pre-processing.

Another set of problems concerns the most part of robots applications are highly dynamic environments, where the objects can change their position with respect to the static environment, those objects can become an obstacle for the robots, for this reason its necessary to map those objects, this relation called DATMO - Detection and Tracking of Moving Objects.

Although these two classification are given in a separate manner, they can be used as complementary to each other as presented in the paper \cite{Wang04a}, which will be dissected later.

\subsection{Map representation}

This is a key point to solve some of the problems related to perception. Thus, the type of representation chosen can be the underline bases to achieve good results.

There exist several possible types of map that can be used to represent the surrounded environment. Among those, three of them gained importance, which are: \textbf{direct}, \textbf{grid} and \textbf{feature-based}. The topological representation is just a derivative type of map and they are not considered in this document.

\subsubsection{Direct}

The direct map representation, is nothing but a straightforward representation of a sensor reading. 

This technique is frequently used in RADAR-like sensors, it depicts the point of impact of each beam (or radio wave frequency) in an image. Binary image is enough to represent such images, it is easy and fast to build, can be extremely oscillating depending on the type of the sensor and the deployment depending on the quality of the sensors.

This representation uses only range measurements (\textit{e.g.} sonar, laser, etc.), this kind of representation is very convenient due to its simplicity. 

Although, building a spatial coherent map from multiple readings is much more complex. As the robot moves in the space, its location is always subject of uncertainties this brings us one big issue, how assembly all the informations captured by the robot if we do not know exactly what was the robot's pose and position? 

A research conducted by university of North York\cite{Lu:1997:GCR:591441.591464} focused their studies in the this issue. The fundamental problem was to obtain a local map from multiple reading, in such way that the resulting map was accurate. Those readings were done by a ego-vehicle able to move in the scenario. The scenario perceived by the robot can be seen in the Figure \ref{fig:mapping:direct:result}.

Those readings were performed by a single robot and its position was calculated based in informations provided by the IMU.

The result can be seen in the Figure \ref{fig:mapping:direct:result}. On the left image we have the original mapping, created with raw information and without any kind of filtering or pre-processing. On the right image, we have a map built with scan alignment technique\cite{Lu:1997:GCR:591441.591464}. On the second image we can see more sharp vertex and a precise alignment in the edges without mispositioned lines.

\begin{figure}[h]
\centering
	\begin{tabular}{lr}\\
		\includegraphics[width=0.5\columnwidth]{img/fig:mapping:direct:a} &
		\includegraphics[width=0.5\columnwidth]{img/fig:mapping:direct:b}
	\end{tabular}
	\caption{Direct mapping result}
	\label{fig:mapping:direct:result}
\end{figure}

Despide of the good result obtained by scan alignment, it is not not necessary in our approach, since we are not worried about building a map of the environment. 

\subsubsection{Feature-based}

Another way to perceive the environment is the feature-based representation. In this type of map we use primitive shapes (\textit{e.g.} circles, lines, dots, ..) to represent the environment and its obstacles.

Several well known methods for detecting those features are available. Hough-transformation, SIFT are good examples. 

Hough-transformations in able to give us equation of lines that are part of the image. One good application for such method is to detect the edges of a road (left and right borders). By having an image of a plane road where the borders reache the infinity, the Hough-transformation is capable to detect the line equation that represents the edges. [put reference]

Scale-invariant feature transform (SIFT) detect similar points between two images, even if this similar points were changes in scale, noise, rotation or illumination. [put reference] 

Those methods are capable of detecting some features of an image and do a partial representation of its characteristics. 

The representation of the environment is not complete and it is subject to failures after applying the previous techniques.

This imprecise representation may generate false interpretation of the environment, that is why applying this kind of representation is not safe when ADAS is the goal platform.

\subsubsection{Grid-based}
\label{ch02:gridbased}

Occupancy Grid is one of these methods and it is largely applied, its built and based on a multidimensional field that maintains the occupancy state information in a cell\cite{Elfes:1989:UOG:68491.68495}. Those cells are built in a regular size, and the number of cells which compose the grid is algorithm dependent.

The changing the size of the cells we can give more or less precision for the representation (reducing or increasing the size of the cell respectively).

At the same time Occupancy Grid (a.k.a. certainty grids) is effective in data representation it is also effective in represent sensor fusion information with it. 

Occupancy grid can be used as well to represent 3D environments, as was demonstrated in a small submersible craft, that looks for old battleships in the bottom of the ocean\cite{DBLP:journals/aim/Moravec88}. The results are interesting to say the least.

One variation of the occupancy filter - The Bayesian Occupancy Filter(BOF) - contains the velocity and the probability distribution in Bayesian framework.

Representing continuous space when dealing with the uncertainty of an action of a mobile robot is extremely demanding in terms of resource usage, either in terms of processing power or memory allocation.

Thus, targeting to amortize the computational requirement for the uncertainty in the displacement of a robot, discretization models are used, Occupancy Grid\cite{Elfes:1989:UOG:68491.68495} is one of the former models proposed to tackle this issue.

This kind of representations is done with multi-dimensional vectors and results in a bird-eye view of the discretized map obtained by the perception sensor Figure \ref{fig:grid:continuous:discretized}.

\begin{figure}[h]
\centering
	\begin{tabular}{lr}\\
		\includegraphics[width=0.25\columnwidth]{img/fig:grid:continuous} &
		\includegraphics[width=0.25\columnwidth]{img/fig:grid:discretized}
	\end{tabular}
	\caption{Continuous \& discretized representation of a map}
	\label{fig:grid:continuous:discretized}
\end{figure}

In grid framework each cell $C$ can be assigned have its state $\phi(C)$ configured to binary domain, so, either the current state of the cell is \textit{occupied} or it is \textit{free}, in this paper we state that the value $1$ represents the occupied state and $0$ a $free$ space Equation \ref{eq:binarycell}. A different domain may be used in the grid, for instance to represent random variable with a probability of occupancy of the cell (Section~\ref{ch03:buildgrid:grid}).

\begin{equation}
P(\phi(C)=1) + P(\phi(C)=0) = 1
\label{eq:binarycell}
\end{equation}

This model can be extended to a more general model coined as \textit{inference grids} that encapsulate multiple properties\cite{Elfes:1989:OGP:916528}.

\textit{Goal: Why occupancy grid? how its built? }

\section{Classification}

Before acting, it is required to understand the scene in the first place.

\textbf{Scenario 1}: In a simpler scenario we can have a robot that moves in an environment, where all objects in its surroundings are static. The robot is the only element present in the environment that is moving.

In this case, the static environment is everything that the robot can perceive. So the robot should use these information to build a map of the navigable path.

\textbf{Scenario 2}: Now, if we pick another scenario: suppose the robot is moving in an overly crowded environment, with different type of objects (cars, pedestrian, trucks, bicycle, motorbikes, etc.) , static and moving objects are mixed. Which object to use as static objects (\textit{e.g.} to build the map) and which to use as dynamic (\textit{e.g.} to calculate the risk of collision)? 

In the \textbf{Scenario 2}, it is much more complicated to obtain any information from the environment.

This lead us to one conclusion: it is hard to do risk analysis when you do not know the objects of the scene.

Thus, as a stepping stone we can perform the dynamic and static environment classification, this solves part of the problem, by classifying the environment in two categories. This will help to:

\begin{enumerate}
\item Build the map
\item Perform risk assessment (Section~\ref{sec:riskassessment})
\end{enumerate}

The scene understanding usually implicates in distinguish different types of objects in the scene. This distinction can be done by using the objects' features(\textit{e.g.} color, shape, etc.), path perceived or even its behavior. 

%multimodels, importance sampling, prior map knowledge

\section{Risk Assessment}
\label{sec:riskassessment}

\textbf{Risk Assessment} is a term adopted in several fields, to indicate the mitigation of the risks involved in the target activity. 

Before describe Risk Assessment and its relation with ADAS, we first, must to understand what risk means.

\textbf{Risk} is anything that can affect (negatively or positively) a subject. By positive meaning that if such thing happens to be true: something good may happen to the subject, or negative meaning if another given thing happens to be true, the subject may suffer negative consequences. It is a common practice to track just the negative risks, that is why positive risk looks so unfamiliar for most of the readers. 

For that reason some literature use terms like hazard or threat to refer to negative risks. But in this document when we refer to risk, we mean the negative risk.

As we saw in certain level of details the meaning of risk, we can understand the whole meaning of the term "Risk assessment" , but first it is better understanding what "assessment" means.

The \textbf{assessment} is the action of study what variations can affect the subject, this variation can be \textbf{direct} or \textbf{indirect} as well, but we will not explain in details, it is sufficient to know that it is related with how the "thing" affected the subject, did it happened directly on the subject? or indirectly?

One good example to illustrate the \textbf{direct} and \textbf{indirect} situations, imagine you bought a very good amount of shares from Apple Inc\copyright. But you forgot that its products are manufactured in an Asian country. Suddenly this Asian country government announces that it is forbidden to export any kind of product to western countries. 

Well, needless to say that you are bankrupt. Something that was not directly related - international diplomacy relations - to your subject can affect dramatically its value. We can refer to the subject as assets, or future assets.

Thus, the subject can be anything with enough degree of importance to justify the risk assessment. Some examples are: a project, stock market share, a schedule, an action.

Risk assessment applied in ADAS is done to evaluate the risk of collision between vehicles. Thus, we are worried about direct risk. 

This is done by perceiving the environment, specifically the dynamic environment. The dynamic environment is chosen to be studied rather than static due to its close relation with  collisions situations. 

The collisions can be caused by several factors, but independent objects moving and sharing the same space are responsible for a big part of the collisions, thus the relationship among dynamic objects are important for the risk assessment.
